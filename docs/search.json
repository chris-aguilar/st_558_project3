[
  {
    "objectID": "EDA.html",
    "href": "EDA.html",
    "title": "EDA",
    "section": "",
    "text": "We’ll be looking at the relationship between diabetes occurrence and several health predictors given in this Diabetes Health Indicators Dataset. The data comes from the Behavioral Risk Factor Surveillance System, a telephone survey collected by the CDC. We’ll be using the 2015 version of the data for exploration, analysis, and machine learning.\nThe data contains information such as the occurrence of diabetes, various diet-related predictors (fruit/veggie consumption, alcohol), behavioral variables (exercise, smoker, doctor visits), demographic information (sex, education, income), and a few subjective variables where respondents are asked about their health.\nWe’ll keep all variables, but be focusing on health/disease history variables, diet variables, a few behavioral variables, sex, and health insurance.\nThe goal of this exploratory data analysis (EDA) is to identify any variables that may be indicative of a higher probability of a respondent being diabetic or pre-diabetic. We’ll get an intuition of this through numerical summaries and visualizations, then test predictive ability of the input variables on Diabetic_binary through machine learning models."
  },
  {
    "objectID": "EDA.html#introduction",
    "href": "EDA.html#introduction",
    "title": "EDA",
    "section": "",
    "text": "We’ll be looking at the relationship between diabetes occurrence and several health predictors given in this Diabetes Health Indicators Dataset. The data comes from the Behavioral Risk Factor Surveillance System, a telephone survey collected by the CDC. We’ll be using the 2015 version of the data for exploration, analysis, and machine learning.\nThe data contains information such as the occurrence of diabetes, various diet-related predictors (fruit/veggie consumption, alcohol), behavioral variables (exercise, smoker, doctor visits), demographic information (sex, education, income), and a few subjective variables where respondents are asked about their health.\nWe’ll keep all variables, but be focusing on health/disease history variables, diet variables, a few behavioral variables, sex, and health insurance.\nThe goal of this exploratory data analysis (EDA) is to identify any variables that may be indicative of a higher probability of a respondent being diabetic or pre-diabetic. We’ll get an intuition of this through numerical summaries and visualizations, then test predictive ability of the input variables on Diabetic_binary through machine learning models."
  },
  {
    "objectID": "EDA.html#ingesting-data",
    "href": "EDA.html#ingesting-data",
    "title": "EDA",
    "section": "Ingesting data",
    "text": "Ingesting data\nWe read data in and select the variables of interest previously mentioned. The description of the data indicates that while all variables are numerically coded, only BMI, GenHlth and PhysHlth are truly numeric. The rest are categorical variables.\nHowever, all the categorical variables except Age, Income, Education, and General Health are binary. So we’ll recategorize these four for levels that make more sense.\n\nlibrary(readr)\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(stringr)\n\ndiabetes2015 &lt;- read_csv(\"data/diabetes_binary_health_indicators_BRFSS2015.csv\")\n\nRows: 253680 Columns: 22\n\n\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (22): Diabetes_binary, HighBP, HighChol, CholCheck, BMI, Smoker, Stroke,...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n# helper function to extract level code and level translation\ngetLevels &lt;- function(x, value_name = \"value\") {\n  # grabbing digits at beginning of string\n  index &lt;- str_extract(x, \"^\\\\d{1,2}\") |&gt; as.integer()\n  # Grabbing everything that starts with a letter onwards\n  value &lt;- str_extract(x, \"[A-Za-z](.+)\")\n  # putting index and value into data frame for joining later\n  res &lt;- data.frame(index = index)\n  res[[value_name]] &lt;- value\n  res\n}\n\n# factor levels to use for some vars, obtained from discussion section using readClipboard()\nages &lt;- c(\"1 Age 18 to 24\", \"2 Age 25 to 29\", \"3 Age 30 to 34\", \"4 Age 35 to 39\", \n\"5 Age 40 to 44\", \"6 Age 45 to 49\", \"7 Age 50 to 54\", \"8 Age 55 to 59\", \n\"9 Age 60 to 64\", \"10 Age 65 to 69\", \"11 Age 70 to 74\", \"12 Age 75 to 79\", \n\"13 Age 80 or older\")\n\nedu &lt;- c(\"1 Never attended school or only kindergarten\", \"2 Grades 1 through 8 (Elementary)\", \n\"3 Grades 9 through 11 (Some high school)\", \"4 Grade 12 or GED (High school graduate)\", \n\"5 College 1 year to 3 years (Some college or technical school)\", \n\"6 College 4 years or more (College graduate)\")\n\nincome &lt;- c(\"1 Less than $10,000\", \"2 Less than $15,000 ($10,000 to less than $15,000)\", \n\"3 Less than $20,000 ($15,000 to less than $20,000)\", \"4 Less than $25,000 ($20,000 to less than $25,000)\", \n\"5 Less than $35,000 ($25,000 to less than $35,000)\", \"6 Less than $50,000 ($35,000 to less than $50,000)\", \n\"7 Less than $75,000 ($50,000 to less than $75,000)\", \"8 More than $75,000\"\n)\n\ngen_health &lt;- c(\"1 excellent\", \"2 very good\", \"3 good\", \"4 fair\", \"5 poor\")\n\n# reference tables\nage_table &lt;- getLevels(ages, \"age_levels\")\nedu_table &lt;- getLevels(edu, \"edu_levels\")\nincome_table &lt;- getLevels(income, \"inc_levels\")\ngen_health_table &lt;- getLevels(gen_health, \"health_levels\")\n\n# Grabbing diabetes data, joining translated categorical variables, and dropping redundant default variables\ndiabetes2015 &lt;- diabetes2015 |&gt;\n  left_join(age_table, by = join_by(Age == index)) |&gt;\n  left_join(edu_table, by = join_by(Education == index)) |&gt;\n  left_join(income_table, by = join_by(Income == index)) |&gt;\n  left_join(gen_health_table, by = join_by(GenHlth == index)) |&gt;\n  select(-Age, -Education, -Income, -GenHlth) |&gt; # dropping baseline variables\n  mutate(Diabetes_binary = ifelse(Diabetes_binary == 1, \"yes\", \"no\")) |&gt;\n  mutate(across(where(is.character), factor)) # making the relevant variables factors"
  },
  {
    "objectID": "EDA.html#eda",
    "href": "EDA.html#eda",
    "title": "EDA",
    "section": "EDA",
    "text": "EDA\nWe’ve cleaned up some categorical variables to have more legible levels, and left the others as-is as we consider the indicator values self-explanatory: 0 if false, 1 if true. If the variable is not binary, then it is considered truly quantitative.\n\nClass imbalance check and NAs\nWe now do some quick explorations of the data. We first check for any NAs. Also, since Diabetes_binary is our response variable of interest, we’ll start EDA with that variable.\n\n# check NA\ndiabetes2015 |&gt; anyNA()\n\n[1] FALSE\n\n# check distribution\ndiabetes2015 |&gt; \n  count(Diabetes_binary) |&gt;\n  mutate(prop = n / sum(n))\n\n# A tibble: 2 × 3\n  Diabetes_binary      n  prop\n  &lt;fct&gt;            &lt;int&gt; &lt;dbl&gt;\n1 no              218334 0.861\n2 yes              35346 0.139\n\n\nDiabetes_binary is binary, as the name suggests, and the prevalence of diabetes in this data set is about 14%, so we exhibit some class imbalance we’ll keep in mind for any modeling. There are no overtly missing values.\n\n\nUnivariate summaries\nNext, we’ll look at univariate summaries.\n\nsummary(diabetes2015)\n\n Diabetes_binary     HighBP         HighChol        CholCheck     \n no :218334      Min.   :0.000   Min.   :0.0000   Min.   :0.0000  \n yes: 35346      1st Qu.:0.000   1st Qu.:0.0000   1st Qu.:1.0000  \n                 Median :0.000   Median :0.0000   Median :1.0000  \n                 Mean   :0.429   Mean   :0.4241   Mean   :0.9627  \n                 3rd Qu.:1.000   3rd Qu.:1.0000   3rd Qu.:1.0000  \n                 Max.   :1.000   Max.   :1.0000   Max.   :1.0000  \n                                                                  \n      BMI            Smoker           Stroke        HeartDiseaseorAttack\n Min.   :12.00   Min.   :0.0000   Min.   :0.00000   Min.   :0.00000     \n 1st Qu.:24.00   1st Qu.:0.0000   1st Qu.:0.00000   1st Qu.:0.00000     \n Median :27.00   Median :0.0000   Median :0.00000   Median :0.00000     \n Mean   :28.38   Mean   :0.4432   Mean   :0.04057   Mean   :0.09419     \n 3rd Qu.:31.00   3rd Qu.:1.0000   3rd Qu.:0.00000   3rd Qu.:0.00000     \n Max.   :98.00   Max.   :1.0000   Max.   :1.00000   Max.   :1.00000     \n                                                                        \n  PhysActivity        Fruits          Veggies       HvyAlcoholConsump\n Min.   :0.0000   Min.   :0.0000   Min.   :0.0000   Min.   :0.0000   \n 1st Qu.:1.0000   1st Qu.:0.0000   1st Qu.:1.0000   1st Qu.:0.0000   \n Median :1.0000   Median :1.0000   Median :1.0000   Median :0.0000   \n Mean   :0.7565   Mean   :0.6343   Mean   :0.8114   Mean   :0.0562   \n 3rd Qu.:1.0000   3rd Qu.:1.0000   3rd Qu.:1.0000   3rd Qu.:0.0000   \n Max.   :1.0000   Max.   :1.0000   Max.   :1.0000   Max.   :1.0000   \n                                                                     \n AnyHealthcare     NoDocbcCost         MentHlth         PhysHlth     \n Min.   :0.0000   Min.   :0.00000   Min.   : 0.000   Min.   : 0.000  \n 1st Qu.:1.0000   1st Qu.:0.00000   1st Qu.: 0.000   1st Qu.: 0.000  \n Median :1.0000   Median :0.00000   Median : 0.000   Median : 0.000  \n Mean   :0.9511   Mean   :0.08418   Mean   : 3.185   Mean   : 4.242  \n 3rd Qu.:1.0000   3rd Qu.:0.00000   3rd Qu.: 2.000   3rd Qu.: 3.000  \n Max.   :1.0000   Max.   :1.00000   Max.   :30.000   Max.   :30.000  \n                                                                     \n    DiffWalk           Sex                age_levels   \n Min.   :0.0000   Min.   :0.0000   Age 60 to 64:33244  \n 1st Qu.:0.0000   1st Qu.:0.0000   Age 65 to 69:32194  \n Median :0.0000   Median :0.0000   Age 55 to 59:30832  \n Mean   :0.1682   Mean   :0.4403   Age 50 to 54:26314  \n 3rd Qu.:0.0000   3rd Qu.:1.0000   Age 70 to 74:23533  \n Max.   :1.0000   Max.   :1.0000   Age 45 to 49:19819  \n                                   (Other)     :87744  \n                                                        edu_levels    \n College 1 year to 3 years (Some college or technical school): 69910  \n College 4 years or more (College graduate)                  :107325  \n Grade 12 or GED (High school graduate)                      : 62750  \n Grades 1 through 8 (Elementary)                             :  4043  \n Grades 9 through 11 (Some high school)                      :  9478  \n Never attended school or only kindergarten                  :   174  \n                                                                      \n                                            inc_levels      health_levels  \n More than $75,000                               :90385   excellent:45299  \n Less than $75,000 ($50,000 to less than $75,000):43219   fair     :31570  \n Less than $50,000 ($35,000 to less than $50,000):36470   good     :75646  \n Less than $35,000 ($25,000 to less than $35,000):25883   poor     :12081  \n Less than $25,000 ($20,000 to less than $25,000):20135   very good:89084  \n Less than $20,000 ($15,000 to less than $20,000):15994                    \n (Other)                                         :21594                    \n\n\nNothing seems out of the ordinary based on the univariate summaries, except for BMI: there’s a max value of 98, which seems really large.\nIn metric units, \\[BMI = mass\\ (kg) / height^2\\  (m)\\] so mass can be calculated as \\[BMI \\times height^2 = mass\\]\nSo if we assume a realistic height of 2 meters with a BMI of 98, one could be estimated to weigh 392 kilograms, or {r 98 * 2^2 * 2.2} pounds. It’s pathologically heavy but not impossible. Heavier weights have been recorded.\n\nlibrary(ggplot2)\n\ndiabetes2015 |&gt;\n  ggplot(aes(x = BMI)) + \n  geom_histogram() +\n  labs(title = \"Histogram of BMIs\")\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n\nWe’ll assume the values are recorded correctly and refrain from deleting records.\n\n\nBivariate summaries, numeric predictors\nWe’ll now check numeric distributions with respect to our binary variable of interest.\n\nlibrary(tidyr)\n\ndiabetes2015 |&gt;\n  select(Diabetes_binary, BMI, MentHlth, PhysHlth) |&gt;\n  pivot_longer(BMI:PhysHlth, names_to = \"Predictor\") |&gt;\n  ggplot(aes(x = value, y = Predictor, col = Diabetes_binary)) +\n  geom_boxplot() +\n  labs(title = \"Distributions of numeric predictors by diabetes indication\")\n\n\n\n\n\n\n\n\nJust eyeballing the results, it looks like BMI may possibly reflect a meaningful difference in diabetes occurrence, as higher BMI values suggest higher diabetes rates.\n\n\nBivariate summaries, binary predictors\nNow we’ll plot differences in binary predictors by diabetes occurrence.\n\ndiabetes2015 |&gt;\n  select(-BMI, -MentHlth, -PhysHlth, -age_levels, -inc_levels, -edu_levels, -health_levels) |&gt;\n  group_by(Diabetes_binary) |&gt;\n  summarize(across(everything(), mean)) |&gt;\n  pivot_longer(!Diabetes_binary, names_to = \"Predictor\") |&gt;\n  ggplot(aes(x = value, y = Predictor, fill = Diabetes_binary)) +\n  geom_col(position = \"dodge\")\n\n\n\n\n\n\n\n\nLooking at predictor rates for both levels of Diabetes_binary, we see that disease/health variables seem to show the largest within-variable difference. These will likely be useful predictors. But just about every predictor except the health insurance/health care cost ones seem to suggest slight differences.\n\n\nBivariate summaries, multi-level categorical predictors\nLastly, we’ll check out Diabetes_binary and counts for the four multi-level categorical predictors.\n\ndiabetes2015 |&gt;\n  select(Diabetes_binary, age_levels, edu_levels, inc_levels, health_levels) |&gt;\n  pivot_longer(!Diabetes_binary, names_to = \"Predictor\") |&gt;\n  count(Diabetes_binary, Predictor, value) |&gt;\n  ggplot(aes(x = n, y = value, fill = Diabetes_binary)) + \n  geom_col(position = \"dodge\") +\n  facet_wrap( ~ Predictor, scales = \"free_y\")\n\n\n\n\n\n\n\n\nThere appear to exist differences in Diabetes_binary for different age, income, health and education groups. Older age groups, lower health, education, and income groups all suggest higher diabetes rates."
  },
  {
    "objectID": "EDA.html#modeling",
    "href": "EDA.html#modeling",
    "title": "EDA",
    "section": "Modeling",
    "text": "Modeling\nNext, we’ll train a few models to predict pre-diabetes or diabetes using some of the variables we just explored.\nClick here for the Modeling page!"
  },
  {
    "objectID": "Modeling.html",
    "href": "Modeling.html",
    "title": "Modeling",
    "section": "",
    "text": "We’ll be looking at the relationship between diabetes occurrence and several health predictors given in this Diabetes Health Indicators Dataset. The data comes from the Behavioral Risk Factor Surveillance System, a telephone survey collected by the CDC. We’ll be using the 2015 version of the data for exploration, analysis, and machine learning.\nThe data contains information such as the occurrence of diabetes, various diet-related predictors (fruit/veggie consumption, alcohol), behavioral variables (exercise, smoker, doctor visits), demographic information (sex, education, income), and a few subjective variables where respondents are asked about their health.\nWe’ll keep all variables, but be focusing on health/disease history variables, diet variables, a few behavioral variables, sex, and health insurance.\nThe goal of this Modeling Exercise is to identify any variables that may be indicative of a higher probability of a respondent being diabetic or pre-diabetic. We’ve gotten an intuition of this through numerical summaries and visualizations. We’ll now test predictive relationships using statistical and machine learning models."
  },
  {
    "objectID": "Modeling.html#introduction",
    "href": "Modeling.html#introduction",
    "title": "Modeling",
    "section": "",
    "text": "We’ll be looking at the relationship between diabetes occurrence and several health predictors given in this Diabetes Health Indicators Dataset. The data comes from the Behavioral Risk Factor Surveillance System, a telephone survey collected by the CDC. We’ll be using the 2015 version of the data for exploration, analysis, and machine learning.\nThe data contains information such as the occurrence of diabetes, various diet-related predictors (fruit/veggie consumption, alcohol), behavioral variables (exercise, smoker, doctor visits), demographic information (sex, education, income), and a few subjective variables where respondents are asked about their health.\nWe’ll keep all variables, but be focusing on health/disease history variables, diet variables, a few behavioral variables, sex, and health insurance.\nThe goal of this Modeling Exercise is to identify any variables that may be indicative of a higher probability of a respondent being diabetic or pre-diabetic. We’ve gotten an intuition of this through numerical summaries and visualizations. We’ll now test predictive relationships using statistical and machine learning models."
  },
  {
    "objectID": "Modeling.html#ingesting-data",
    "href": "Modeling.html#ingesting-data",
    "title": "Modeling",
    "section": "Ingesting data",
    "text": "Ingesting data\nWe read in and clean up the data as done during the EDA phase.\n\nlibrary(readr)\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(stringr)\n\ndiabetes2015 &lt;- read_csv(\"data/diabetes_binary_health_indicators_BRFSS2015.csv\")\n\nRows: 253680 Columns: 22\n\n\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (22): Diabetes_binary, HighBP, HighChol, CholCheck, BMI, Smoker, Stroke,...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n# helper function to extract level code and level translation\ngetLevels &lt;- function(x, value_name = \"value\") {\n  # grabbing digits at beginning of string\n  index &lt;- str_extract(x, \"^\\\\d{1,2}\") |&gt; as.integer()\n  # Grabbing everything that starts with a letter onwards\n  value &lt;- str_extract(x, \"[A-Za-z](.+)\")\n  # putting index and value into data frame for joining later\n  res &lt;- data.frame(index = index)\n  res[[value_name]] &lt;- value\n  res\n}\n\n# factor levels to use for some vars, obtained from discussion section using readClipboard()\nages &lt;- c(\"1 Age 18 to 24\", \"2 Age 25 to 29\", \"3 Age 30 to 34\", \"4 Age 35 to 39\", \n\"5 Age 40 to 44\", \"6 Age 45 to 49\", \"7 Age 50 to 54\", \"8 Age 55 to 59\", \n\"9 Age 60 to 64\", \"10 Age 65 to 69\", \"11 Age 70 to 74\", \"12 Age 75 to 79\", \n\"13 Age 80 or older\")\n\nedu &lt;- c(\"1 Never attended school or only kindergarten\", \"2 Grades 1 through 8 (Elementary)\", \n\"3 Grades 9 through 11 (Some high school)\", \"4 Grade 12 or GED (High school graduate)\", \n\"5 College 1 year to 3 years (Some college or technical school)\", \n\"6 College 4 years or more (College graduate)\")\n\nincome &lt;- c(\"1 Less than $10,000\", \"2 Less than $15,000 ($10,000 to less than $15,000)\", \n\"3 Less than $20,000 ($15,000 to less than $20,000)\", \"4 Less than $25,000 ($20,000 to less than $25,000)\", \n\"5 Less than $35,000 ($25,000 to less than $35,000)\", \"6 Less than $50,000 ($35,000 to less than $50,000)\", \n\"7 Less than $75,000 ($50,000 to less than $75,000)\", \"8 More than $75,000\"\n)\n\ngen_health &lt;- c(\"1 excellent\", \"2 very good\", \"3 good\", \"4 fair\", \"5 poor\")\n\n# reference tables\nage_table &lt;- getLevels(ages, \"age_levels\")\nedu_table &lt;- getLevels(edu, \"edu_levels\")\nincome_table &lt;- getLevels(income, \"inc_levels\")\ngen_health_table &lt;- getLevels(gen_health, \"health_levels\")\n\n# Grabbing diabetes data, joining translated categorical variables, and dropping redundant default variables\ndiabetes2015 &lt;- diabetes2015 |&gt;\n  left_join(age_table, by = join_by(Age == index)) |&gt;\n  left_join(edu_table, by = join_by(Education == index)) |&gt;\n  left_join(income_table, by = join_by(Income == index)) |&gt;\n  left_join(gen_health_table, by = join_by(GenHlth == index)) |&gt;\n  select(-Age, -Education, -Income, -GenHlth) |&gt; # dropping baseline variables\n  mutate(Diabetes_binary = ifelse(Diabetes_binary == 1, \"yes\", \"no\")) |&gt;\n  mutate(across(where(is.character), factor)) # making the relevant variables factors"
  },
  {
    "objectID": "Modeling.html#traintest-split",
    "href": "Modeling.html#traintest-split",
    "title": "Modeling",
    "section": "Train/Test split",
    "text": "Train/Test split\nNext, we’ll split the data into a training and test set. We’ll be doing a 70/30 split for the training/test set, and we’ll do 5-fold cross validation.\nWe’ll be using logLoss as our metric for model evaluation, which should be minimized.\n\nWhy log loss?\nFrom the yardstick package’s documentation for mn_log_loss:\n\nLog loss is a measure of the performance of a classification model. A perfect model has a log loss of 0. Compared with accuracy(), log loss takes into account the uncertainty in the prediction and gives a more detailed view into the actual performance. For example, given two input probabilities of .6 and .9 where both are classified as predicting a positive value, say, “Yes”, the accuracy metric would interpret them as having the same value. If the true output is “Yes”, log loss penalizes .6 because it is “less sure” of it’s result compared to the probability of .9.\n\nSo essentially, when we have a binary response variable, we may prefer log loss to something like accuracy because it will penalize “unsure” probability predictions. This gives us more insight into how well a model is predicting instead of just giving us a “positive/”negative” class. This becomes especially important when our classes are imbalanced in a binary outcome response, as predicting the minority class gets more difficult.\n\nlibrary(caret)\n\nWarning: package 'caret' was built under R version 4.4.1\n\n\nLoading required package: ggplot2\n\n\nLoading required package: lattice\n\nset.seed(123)\ntrain_split &lt;- createDataPartition(diabetes2015$Diabetes_binary, p = 0.7, list = FALSE)\n\ntrain_set &lt;- diabetes2015[train_split, ]\ntest_set &lt;- diabetes2015[-train_split, ]\n\n# 5 fold cross validation and log loss for train control\ncontrol &lt;- trainControl(method=\"cv\", number=5, classProbs=TRUE, summaryFunction=mnLogLoss)\n\nWe’ll now begin considering different model classes and fitting models to determine which one will perform best on the test set."
  },
  {
    "objectID": "Modeling.html#model-fitting",
    "href": "Modeling.html#model-fitting",
    "title": "Modeling",
    "section": "Model fitting",
    "text": "Model fitting\nWe’ll be considering three different classes of models:\n\nLogistic regression\nClassification trees\nRandom forests\n\nWe begin with logistic regression.\n\nLogistic regression\nLogistic regression is a generalized linear model that relates a binary response to a linear function of predictors. This relation happens through a link function. For logistic regression, we’ll typically be using the logit function, though other options exist.\nWe would apply a logistic regression to a binary response because, while we could use ordinary least squares on a 0/1 outcome, the nature of that model would mean we may predict probabilities less than 0 or greater than 1, which wouldn’t be appropriate – linear regression assumes the outcome’s support is all real numbers. Logistic regression restricts the probability predictions to be between 0 and 1. However, we must be careful because we no longer interpret logistic regression coefficients the way we would for linear regression. By default, we interpret them in terms of log-odds.\nLet’s fit some logistic regression models. We’ll have three sets of variables: objective health data as inputs (stuff that a doctor would provide), subjective data as inputs (stuff from questionnaires plus demographics), and all data.\nWe’ll visually compare performance across resamples.\n\nset.seed(123)\n# More \"objective\" health variables\nlogreg1 &lt;- train(Diabetes_binary ~ HighBP + HighChol + BMI + Stroke + HeartDiseaseorAttack + DiffWalk,\n                 data = train_set,\n                 method = \"glm\",\n                 family = \"binomial\",\n                 trControl = control,\n                 metric = \"logLoss\")\n\n# Subjective vars and demographic data\nlogreg2 &lt;- train(Diabetes_binary ~ \n                   CholCheck + \n                   Smoker + \n                   PhysActivity + \n                   Fruits + \n                   Veggies + \n                   HvyAlcoholConsump + \n                   AnyHealthcare +\n                   NoDocbcCost +\n                   MentHlth +\n                   PhysHlth + \n                   Sex +\n                   age_levels +\n                   edu_levels +\n                   inc_levels + \n                   health_levels,\n                 data = train_set,\n                 method = \"glm\",\n                 family = \"binomial\",\n                 trControl = control,\n                 metric = \"logLoss\")\n\n# all variables\nlogreg3 &lt;- train(Diabetes_binary ~ \n                   .,\n                 data = train_set,\n                 method = \"glm\",\n                 family = \"binomial\",\n                 trControl = control,\n                 metric = \"logLoss\")\n\n# log reg resamples\nlogreg_resamples &lt;- resamples(list(health_data = logreg1, questionnaire_data = logreg2, all_data = logreg3))\n\nbwplot(logreg_resamples)\n\n\n\n\n\n\n\n\nAfter training all three of our candidate models, the model using all the predictors minimizes log loss. However, the difference between the best logistic regression model and the simplest logistic regression model isn’t too big… so we’ll use the full model and the simplest model captured by health_data on the test set and see what the difference is.\nFor curiousity’s sake, let’s look at a summary of that best logistic regression model, and the simplest model.\n\n# simplest model\nsummary(logreg1)\n\n\nCall:\nNULL\n\nCoefficients:\n                      Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)          -4.802712   0.033616 -142.87   &lt;2e-16 ***\nHighBP                1.067853   0.016874   63.28   &lt;2e-16 ***\nHighChol              0.705505   0.015789   44.68   &lt;2e-16 ***\nBMI                   0.058623   0.001014   57.80   &lt;2e-16 ***\nStroke                0.337095   0.029829   11.30   &lt;2e-16 ***\nHeartDiseaseorAttack  0.580964   0.020494   28.35   &lt;2e-16 ***\nDiffWalk              0.684355   0.016986   40.29   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 143396  on 177576  degrees of freedom\nResidual deviance: 120795  on 177570  degrees of freedom\nAIC: 120809\n\nNumber of Fisher Scoring iterations: 5\n\n# full model\nsummary(logreg3)\n\n\nCall:\nNULL\n\nCoefficients:\n                                                              Estimate\n(Intercept)                                                  -8.217491\nHighBP                                                        0.693213\nHighChol                                                      0.525835\nCholCheck                                                     1.261563\nBMI                                                           0.059036\nSmoker                                                       -0.039468\nStroke                                                        0.152456\nHeartDiseaseorAttack                                          0.241201\nPhysActivity                                                 -0.059418\nFruits                                                       -0.008333\nVeggies                                                      -0.028423\nHvyAlcoholConsump                                            -0.761624\nAnyHealthcare                                                 0.087593\nNoDocbcCost                                                  -0.007637\nMentHlth                                                     -0.003463\nPhysHlth                                                     -0.003196\nDiffWalk                                                      0.151527\nSex                                                           0.266505\n`age_levelsAge 25 to 29`                                      0.118075\n`age_levelsAge 30 to 34`                                      0.422357\n`age_levelsAge 35 to 39`                                      0.839950\n`age_levelsAge 40 to 44`                                      1.081653\n`age_levelsAge 45 to 49`                                      1.313777\n`age_levelsAge 50 to 54`                                      1.557893\n`age_levelsAge 55 to 59`                                      1.610480\n`age_levelsAge 60 to 64`                                      1.841716\n`age_levelsAge 65 to 69`                                      2.007648\n`age_levelsAge 70 to 74`                                      2.055305\n`age_levelsAge 75 to 79`                                      1.974004\n`age_levelsAge 80 or older`                                   1.797678\n`edu_levelsCollege 4 years or more (College graduate)`       -0.090230\n`edu_levelsGrade 12 or GED (High school graduate)`           -0.036568\n`edu_levelsGrades 1 through 8 (Elementary)`                   0.142040\n`edu_levelsGrades 9 through 11 (Some high school)`           -0.001561\n`edu_levelsNever attended school or only kindergarten`        0.137789\n`inc_levelsLess than $15,000 ($10,000 to less than $15,000)` -0.022839\n`inc_levelsLess than $20,000 ($15,000 to less than $20,000)` -0.044046\n`inc_levelsLess than $25,000 ($20,000 to less than $25,000)` -0.093504\n`inc_levelsLess than $35,000 ($25,000 to less than $35,000)` -0.147777\n`inc_levelsLess than $50,000 ($35,000 to less than $50,000)` -0.236534\n`inc_levelsLess than $75,000 ($50,000 to less than $75,000)` -0.261493\n`inc_levelsMore than $75,000`                                -0.395796\nhealth_levelsfair                                             1.821581\nhealth_levelsgood                                             1.394845\nhealth_levelspoor                                             1.970615\n`health_levelsvery good`                                      0.714836\n                                                             Std. Error z value\n(Intercept)                                                    0.174246 -47.160\nHighBP                                                         0.017618  39.346\nHighChol                                                       0.016246  32.366\nCholCheck                                                      0.082915  15.215\nBMI                                                            0.001100  53.645\nSmoker                                                         0.015878  -2.486\nStroke                                                         0.030031   5.077\nHeartDiseaseorAttack                                           0.021263  11.344\nPhysActivity                                                   0.017247  -3.445\nFruits                                                         0.016405  -0.508\nVeggies                                                        0.019053  -1.492\nHvyAlcoholConsump                                              0.045809 -16.626\nAnyHealthcare                                                  0.040517   2.162\nNoDocbcCost                                                    0.027657  -0.276\nMentHlth                                                       0.001022  -3.390\nPhysHlth                                                       0.000961  -3.326\nDiffWalk                                                       0.020289   7.468\nSex                                                            0.016110  16.543\n`age_levelsAge 25 to 29`                                       0.173114   0.682\n`age_levelsAge 30 to 34`                                       0.154799   2.728\n`age_levelsAge 35 to 39`                                       0.146866   5.719\n`age_levelsAge 40 to 44`                                       0.143526   7.536\n`age_levelsAge 45 to 49`                                       0.141301   9.298\n`age_levelsAge 50 to 54`                                       0.139865  11.139\n`age_levelsAge 55 to 59`                                       0.139443  11.549\n`age_levelsAge 60 to 64`                                       0.139154  13.235\n`age_levelsAge 65 to 69`                                       0.139169  14.426\n`age_levelsAge 70 to 74`                                       0.139654  14.717\n`age_levelsAge 75 to 79`                                       0.140456  14.054\n`age_levelsAge 80 or older`                                    0.140670  12.779\n`edu_levelsCollege 4 years or more (College graduate)`         0.020413  -4.420\n`edu_levelsGrade 12 or GED (High school graduate)`             0.020070  -1.822\n`edu_levelsGrades 1 through 8 (Elementary)`                    0.049895   2.847\n`edu_levelsGrades 9 through 11 (Some high school)`             0.036107  -0.043\n`edu_levelsNever attended school or only kindergarten`         0.235537   0.585\n`inc_levelsLess than $15,000 ($10,000 to less than $15,000)`   0.042470  -0.538\n`inc_levelsLess than $20,000 ($15,000 to less than $20,000)`   0.040925  -1.076\n`inc_levelsLess than $25,000 ($20,000 to less than $25,000)`   0.040204  -2.326\n`inc_levelsLess than $35,000 ($25,000 to less than $35,000)`   0.039605  -3.731\n`inc_levelsLess than $50,000 ($35,000 to less than $50,000)`   0.039087  -6.051\n`inc_levelsLess than $75,000 ($50,000 to less than $75,000)`   0.039510  -6.618\n`inc_levelsMore than $75,000`                                  0.039193 -10.099\nhealth_levelsfair                                              0.042083  43.286\nhealth_levelsgood                                              0.038809  35.941\nhealth_levelspoor                                              0.050397  39.102\n`health_levelsvery good`                                       0.039650  18.029\n                                                             Pr(&gt;|z|)    \n(Intercept)                                                   &lt; 2e-16 ***\nHighBP                                                        &lt; 2e-16 ***\nHighChol                                                      &lt; 2e-16 ***\nCholCheck                                                     &lt; 2e-16 ***\nBMI                                                           &lt; 2e-16 ***\nSmoker                                                       0.012929 *  \nStroke                                                       3.84e-07 ***\nHeartDiseaseorAttack                                          &lt; 2e-16 ***\nPhysActivity                                                 0.000571 ***\nFruits                                                       0.611487    \nVeggies                                                      0.135754    \nHvyAlcoholConsump                                             &lt; 2e-16 ***\nAnyHealthcare                                                0.030628 *  \nNoDocbcCost                                                  0.782435    \nMentHlth                                                     0.000700 ***\nPhysHlth                                                     0.000883 ***\nDiffWalk                                                     8.12e-14 ***\nSex                                                           &lt; 2e-16 ***\n`age_levelsAge 25 to 29`                                     0.495198    \n`age_levelsAge 30 to 34`                                     0.006364 ** \n`age_levelsAge 35 to 39`                                     1.07e-08 ***\n`age_levelsAge 40 to 44`                                     4.84e-14 ***\n`age_levelsAge 45 to 49`                                      &lt; 2e-16 ***\n`age_levelsAge 50 to 54`                                      &lt; 2e-16 ***\n`age_levelsAge 55 to 59`                                      &lt; 2e-16 ***\n`age_levelsAge 60 to 64`                                      &lt; 2e-16 ***\n`age_levelsAge 65 to 69`                                      &lt; 2e-16 ***\n`age_levelsAge 70 to 74`                                      &lt; 2e-16 ***\n`age_levelsAge 75 to 79`                                      &lt; 2e-16 ***\n`age_levelsAge 80 or older`                                   &lt; 2e-16 ***\n`edu_levelsCollege 4 years or more (College graduate)`       9.86e-06 ***\n`edu_levelsGrade 12 or GED (High school graduate)`           0.068451 .  \n`edu_levelsGrades 1 through 8 (Elementary)`                  0.004417 ** \n`edu_levelsGrades 9 through 11 (Some high school)`           0.965520    \n`edu_levelsNever attended school or only kindergarten`       0.558547    \n`inc_levelsLess than $15,000 ($10,000 to less than $15,000)` 0.590735    \n`inc_levelsLess than $20,000 ($15,000 to less than $20,000)` 0.281807    \n`inc_levelsLess than $25,000 ($20,000 to less than $25,000)` 0.020032 *  \n`inc_levelsLess than $35,000 ($25,000 to less than $35,000)` 0.000191 ***\n`inc_levelsLess than $50,000 ($35,000 to less than $50,000)` 1.44e-09 ***\n`inc_levelsLess than $75,000 ($50,000 to less than $75,000)` 3.63e-11 ***\n`inc_levelsMore than $75,000`                                 &lt; 2e-16 ***\nhealth_levelsfair                                             &lt; 2e-16 ***\nhealth_levelsgood                                             &lt; 2e-16 ***\nhealth_levelspoor                                             &lt; 2e-16 ***\n`health_levelsvery good`                                      &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 143396  on 177576  degrees of freedom\nResidual deviance: 112982  on 177531  degrees of freedom\nAIC: 113074\n\nNumber of Fisher Scoring iterations: 6\n\n\nPretty interesting.\nNow we’ll look at classification trees.\n\n\nClassification trees\nClassification trees are non-parametric models that make predictions by partitioning the predictor space such that some loss function of interest is minimized. Predictions for the partitions, in the classification case, are simply the most prevalent class (or the proportion of that most prevalent class, if we’re predicting probabilities) within that partition. Classification trees can easily overfit data, so we adjust their performance by tuning a complexity parameter cp for these models.\nWe might use classification trees when we are unsure of what variables have useful relationships with the response or when we are unsure of the exact nature of the relationship, when we are unsure of any interaction effects, and when we are interested in feature selection while still maintaining a degree of interpretability. This is only applicable with smaller predictor sets however. Also, classification tree results can be highly variable with small changes in the data, so this is another consideration we must take.\nLet’s fit some trees! We’ll plot the logLoss value per value of cp, the complexity parameter to get an idea of which tree does best. We won’t be visualizing a tree diagram though, as with all the variables we have, it’ll be pretty difficult to interpret.\n\ntree_grid &lt;- data.frame(cp = seq(from = 0.0001, to = 0.0015, by = 0.0001))\n\nset.seed(456)\nrpart_fit &lt;- train(Diabetes_binary ~ ., data = train_set,\n                  trControl = control, \n                  method = \"rpart\", \n                  metric = \"logLoss\",\n                  tuneGrid = tree_grid)\n\nplot(rpart_fit)\n\n\n\n\n\n\n\n\nWe see our estimate of best rpart model is at a cp value of 2^{-4}.\nFinally, we’ll look at random forest models, which are ensembles of individual tree models.\n\n\nRandom forest\nPLEASE NOTE: Given the size of the data, instead of method = 'rf', we’ll be using method = 'ranger' to use the ranger package’s implementation of the random forest algorithm. It’s much faster, and while going through this assignment, it took about 20 minutes to finish 3-fold CV over 500 trees, 3 values of mtry, and 2 values of splitrule. A prior approach on rf took about 2 hours for 5 values of mtry and 5-fold CV on 300 trees.\nRandom forest models are ensemble models. This means they use many submodels to make “averaged” predictions. The benefit here is that the prediction accuracy is typically improved by virtue of reduced variance of individual tree predictions, since predictions are aggregated from several trees.\nRandom forests take a bootstrapped sample, the same size as the actual sample, and train a tree on this resample. Then this tree calls predictions for some given x-values. This process is repeated a set number of times, typically 100 or 1000. Finally, those 100 or 1000 sets of predictions are averaged. This is the bagged tree approach.\nThe extra step random forests add is random sampling of the predictors to decorrelate the trees. This makes it so that correlated predictors don’t dominate the predictor space.\nWe’ll train some random forest models and select the best one based on the best mtry parameter. The ranger implementation of random forest, by default, uses 500 trees. We’ll use all predictors as initial inputs. Due to the length of time it takes to train a random forest model, we’ll be reducing the number of cross-validation folds to 3, and let the caret’s train function randomly select hyperparameter values to tune over.\n\n# 3 fold cross validation and log loss for train control\nrf_control &lt;- trainControl(method=\"cv\", number=3, classProbs=TRUE, summaryFunction=mnLogLoss)\n\nset.seed(456)\nrf_fit &lt;- train(Diabetes_binary ~ ., data = train_set,\n                  trControl = rf_control, \n                  method = \"ranger\", \n                  metric = \"logLoss\",\n                  tuneLength = 3\n                  # tuneGrid = tree_grid\n                )\n\nGrowing trees.. Progress: 42%. Estimated remaining time: 42 seconds.\nGrowing trees.. Progress: 86%. Estimated remaining time: 10 seconds.\nGrowing trees.. Progress: 22%. Estimated remaining time: 1 minute, 47 seconds.\nGrowing trees.. Progress: 46%. Estimated remaining time: 1 minute, 11 seconds.\nGrowing trees.. Progress: 71%. Estimated remaining time: 39 seconds.\nGrowing trees.. Progress: 94%. Estimated remaining time: 8 seconds.\nGrowing trees.. Progress: 36%. Estimated remaining time: 55 seconds.\nGrowing trees.. Progress: 73%. Estimated remaining time: 23 seconds.\nGrowing trees.. Progress: 19%. Estimated remaining time: 2 minutes, 17 seconds.\nGrowing trees.. Progress: 39%. Estimated remaining time: 1 minute, 38 seconds.\nGrowing trees.. Progress: 60%. Estimated remaining time: 1 minute, 4 seconds.\nGrowing trees.. Progress: 80%. Estimated remaining time: 32 seconds.\nGrowing trees.. Progress: 99%. Estimated remaining time: 1 seconds.\nGrowing trees.. Progress: 42%. Estimated remaining time: 42 seconds.\nGrowing trees.. Progress: 85%. Estimated remaining time: 10 seconds.\nGrowing trees.. Progress: 22%. Estimated remaining time: 1 minute, 47 seconds.\nGrowing trees.. Progress: 47%. Estimated remaining time: 1 minute, 11 seconds.\nGrowing trees.. Progress: 71%. Estimated remaining time: 38 seconds.\nGrowing trees.. Progress: 95%. Estimated remaining time: 7 seconds.\nGrowing trees.. Progress: 36%. Estimated remaining time: 55 seconds.\nGrowing trees.. Progress: 73%. Estimated remaining time: 23 seconds.\nGrowing trees.. Progress: 19%. Estimated remaining time: 2 minutes, 10 seconds.\nGrowing trees.. Progress: 38%. Estimated remaining time: 1 minute, 39 seconds.\nGrowing trees.. Progress: 58%. Estimated remaining time: 1 minute, 6 seconds.\nGrowing trees.. Progress: 78%. Estimated remaining time: 34 seconds.\nGrowing trees.. Progress: 99%. Estimated remaining time: 2 seconds.\nGrowing trees.. Progress: 42%. Estimated remaining time: 42 seconds.\nGrowing trees.. Progress: 85%. Estimated remaining time: 10 seconds.\nGrowing trees.. Progress: 22%. Estimated remaining time: 1 minute, 49 seconds.\nGrowing trees.. Progress: 46%. Estimated remaining time: 1 minute, 12 seconds.\nGrowing trees.. Progress: 70%. Estimated remaining time: 39 seconds.\nGrowing trees.. Progress: 93%. Estimated remaining time: 9 seconds.\nGrowing trees.. Progress: 35%. Estimated remaining time: 56 seconds.\nGrowing trees.. Progress: 72%. Estimated remaining time: 24 seconds.\nGrowing trees.. Progress: 19%. Estimated remaining time: 2 minutes, 10 seconds.\nGrowing trees.. Progress: 38%. Estimated remaining time: 1 minute, 39 seconds.\nGrowing trees.. Progress: 58%. Estimated remaining time: 1 minute, 7 seconds.\nGrowing trees.. Progress: 78%. Estimated remaining time: 35 seconds.\nGrowing trees.. Progress: 97%. Estimated remaining time: 4 seconds.\n\nplot(rf_fit)\n\n\n\n\n\n\n\n\nWe see from the plot of hyperparameter value relationships on CV logLoss that fewer predictors and a gini split-rule yield the best RF model from the ranger package.\nTime to evaluate performance on the test set!\n\n\nTest set evaluation\nWe’ll create test set predictions for the full-variable logistic regression model, for the simplest logistic regression model, for the best classification tree model, and for the best random forest model. Then we’ll determine which model performs best overall, but note that we’ll select the simplest model if there isn’t much difference in performance between the best and second best model.\nWe’ll create a helper function getLogLoss to evaluate model performance on the test set, then stick the results in a data frame and see which model gets the lowest test set log loss.\n\ngetLogLoss &lt;- function(model_fit, newdata) {\n  \n  # predict classes\n  class_preds &lt;- predict(model_fit, newdata = newdata)\n  classes &lt;- levels(class_preds)\n  \n  # predict probabilities\n  prob_preds &lt;- predict(model_fit, newdata = newdata, type = \"prob\")\n  \n  # put data in data frame with required columns for mnLogLoss function\n  preds_df &lt;- data.frame(obs = newdata$Diabetes_binary, pred = class_preds) |&gt; bind_cols(prob_preds)\n  \n  # get log loss\n  mnLogLoss(preds_df, lev = classes)\n}\n\nmodel_performance &lt;- data.frame(\n  models = c(\"full logistic regression\", \"simplest logistic regression\", \"class. tree\", \"random forest\"),\n  test_log_loss = c(\n    getLogLoss(logreg3, test_set), \n    getLogLoss(logreg1, test_set), \n    getLogLoss(rpart_fit, test_set), \n    getLogLoss(rf_fit, test_set))\n  )\n\n# Viewing model results by lowest log loss\nmodel_performance |&gt; arrange(test_log_loss)\n\n                        models test_log_loss\n1     full logistic regression     0.3143462\n2                random forest     0.3276786\n3 simplest logistic regression     0.3369585\n4                  class. tree     0.3556284\n\n\nWe see the best model, evaluated on the test set, is full logistic regression!\nThe differences between the models aren’t huge, though, and we prefer simplicity, so going forward, we’ll choose the simplest logistic regression model for our API. Please note that if we absolutely had to get the most accurate model, we’d likely devote more time to hyperparameter tuning on random forest, or we’d take the full logistic regression model."
  },
  {
    "objectID": "Modeling.html#modeling-conclusion",
    "href": "Modeling.html#modeling-conclusion",
    "title": "Modeling",
    "section": "Modeling conclusion",
    "text": "Modeling conclusion\nWe trained 3 logistic regression models with different predictor sets, a few classification trees, and a few random forest models using cross validation. Then we evaluated their performance on the test set. We found full logistic regression performs best.\nBut as we saw, the differences in test set performance aren’t massive, and we prefer parsimony in our modeling, so we’ll use the simplest logistic regression for our API.\nThank you for reading!"
  }
]