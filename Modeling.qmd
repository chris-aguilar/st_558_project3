---
title: "Modeling"
author: "Chris Aguilar"
format: html
editor: visual
---

## Introduction

We'll be looking at the relationship between diabetes occurrence and several health predictors given in this [Diabetes Health Indicators Dataset](https://www.kaggle.com/datasets/alexteboul/diabetes-health-indicators-dataset/). The data comes from the Behavioral Risk Factor Surveillance System, a telephone survey collected by the CDC. We'll be using the 2015 version of the data for exploration, analysis, and machine learning.

The data contains information such as the occurrence of diabetes, various diet-related predictors (fruit/veggie consumption, alcohol), behavioral variables (exercise, smoker, doctor visits), demographic information (sex, education, income), and a few subjective variables where respondents are asked about their health.

We'll keep all variables, but be focusing on health/disease history variables, diet variables, a few behavioral variables, sex, and health insurance.

The goal of this **Modeling Exercise** is to identify any variables that may be indicative of a higher probability of a respondent being diabetic or pre-diabetic. We've gotten an intuition of this through numerical summaries and visualizations. We'll now test predictive relationships using statistical and machine learning models.

## Ingesting data

We read in and clean up the data as done during the **EDA** phase.

```{r read prep data}
library(readr)
library(dplyr)
library(stringr)

diabetes2015 <- read_csv("data/diabetes_binary_health_indicators_BRFSS2015.csv")

# helper function to extract level code and level translation
getLevels <- function(x, value_name = "value") {
  # grabbing digits at beginning of string
  index <- str_extract(x, "^\\d{1,2}") |> as.integer()
  # Grabbing everything that starts with a letter onwards
  value <- str_extract(x, "[A-Za-z](.+)")
  # putting index and value into data frame for joining later
  res <- data.frame(index = index)
  res[[value_name]] <- value
  res
}

# factor levels to use for some vars, obtained from discussion section using readClipboard()
ages <- c("1 Age 18 to 24", "2 Age 25 to 29", "3 Age 30 to 34", "4 Age 35 to 39", 
"5 Age 40 to 44", "6 Age 45 to 49", "7 Age 50 to 54", "8 Age 55 to 59", 
"9 Age 60 to 64", "10 Age 65 to 69", "11 Age 70 to 74", "12 Age 75 to 79", 
"13 Age 80 or older")

edu <- c("1 Never attended school or only kindergarten", "2 Grades 1 through 8 (Elementary)", 
"3 Grades 9 through 11 (Some high school)", "4 Grade 12 or GED (High school graduate)", 
"5 College 1 year to 3 years (Some college or technical school)", 
"6 College 4 years or more (College graduate)")

income <- c("1 Less than $10,000", "2 Less than $15,000 ($10,000 to less than $15,000)", 
"3 Less than $20,000 ($15,000 to less than $20,000)", "4 Less than $25,000 ($20,000 to less than $25,000)", 
"5 Less than $35,000 ($25,000 to less than $35,000)", "6 Less than $50,000 ($35,000 to less than $50,000)", 
"7 Less than $75,000 ($50,000 to less than $75,000)", "8 More than $75,000"
)

gen_health <- c("1 excellent", "2 very good", "3 good", "4 fair", "5 poor")

# reference tables
age_table <- getLevels(ages, "age_levels")
edu_table <- getLevels(edu, "edu_levels")
income_table <- getLevels(income, "inc_levels")
gen_health_table <- getLevels(gen_health, "health_levels")

# Grabbing diabetes data, joining translated categorical variables, and dropping redundant default variables
diabetes2015 <- diabetes2015 |>
  left_join(age_table, by = join_by(Age == index)) |>
  left_join(edu_table, by = join_by(Education == index)) |>
  left_join(income_table, by = join_by(Income == index)) |>
  left_join(gen_health_table, by = join_by(GenHlth == index)) |>
  select(-Age, -Education, -Income, -GenHlth) |> # dropping baseline variables
  mutate(Diabetes_binary = ifelse(Diabetes_binary == 1, "yes", "no")) |>
  mutate(across(where(is.character), factor)) # making the relevant variables factors

```

## Train/Test split

Next, we'll split the data into a training and test set. We'll be doing a 70/30 split for the training/test set, and we'll do 5-fold cross validation.

We'll be using `logLoss` as our metric for model evaluation, which should be minimized.

### Why log loss?

From the [yardstick package's documentation](https://yardstick.tidymodels.org/reference/mn_log_loss.html#details) for `mn_log_loss`:

> Log loss is a measure of the performance of a classification model. A perfect model has a log loss of 0. Compared with accuracy(), log loss takes into account the uncertainty in the prediction and gives a more detailed view into the actual performance. For example, given two input probabilities of .6 and .9 where both are classified as predicting a positive value, say, "Yes", the accuracy metric would interpret them as having the same value. If the true output is "Yes", log loss penalizes .6 because it is "less sure" of it's result compared to the probability of .9.

So essentially, when we have a binary response variable, we may prefer log loss to something like accuracy because it will penalize "unsure" probability predictions. This gives us more insight into how well a model is predicting instead of just giving us a "positive/"negative" class. This becomes especially important when our classes are imbalanced in a binary outcome response, as predicting the minority class gets more difficult.

```{r train test split}
library(caret)

set.seed(123)
train_split <- createDataPartition(diabetes2015$Diabetes_binary, p = 0.7, list = FALSE)

train_set <- diabetes2015[train_split, ]
test_set <- diabetes2015[-train_split, ]

# 5 fold cross validation and log loss for train control
control <- trainControl(method="cv", number=5, classProbs=TRUE, summaryFunction=mnLogLoss)
```

We'll now begin considering different model classes and fitting models to determine which one will perform best on the test set.

## Model fitting

We'll be considering three different classes of models:

-   Logistic regression
-   Classification trees
-   Random forests

We begin with logistic regression.

### Logistic regression

Logistic regression is a generalized linear model that relates a binary response to a linear function of predictors. This relation happens through a link function. For logistic regression, we'll typically be using the logit function, though other options exist.

We would apply a logistic regression to a binary response because, while we **could** use ordinary least squares on a 0/1 outcome, the nature of that model would mean we may predict probabilities less than 0 or greater than 1, which wouldn't be appropriate -- linear regression assumes the outcome's support is all real numbers. Logistic regression restricts the probability predictions to be between 0 and 1. However, we must be careful because we no longer interpret logistic regression coefficients the way we would for linear regression. By default, we interpret them in terms of log-odds.

Let's fit some logistic regression models. We'll have three sets of variables: **objective health data** as inputs (stuff that a doctor would provide), **subjective data** as inputs (stuff from questionnaires plus demographics), and **all data**.

We'll visually compare performance across resamples.

```{r logreg}

set.seed(123)
# More "objective" health variables
logreg1 <- train(Diabetes_binary ~ HighBP + HighChol + BMI + Stroke + HeartDiseaseorAttack + DiffWalk,
                 data = train_set,
                 method = "glm",
                 family = "binomial",
                 trControl = control,
                 metric = "logLoss")

# Subjective vars and demographic data
logreg2 <- train(Diabetes_binary ~ 
                   CholCheck + 
                   Smoker + 
                   PhysActivity + 
                   Fruits + 
                   Veggies + 
                   HvyAlcoholConsump + 
                   AnyHealthcare +
                   NoDocbcCost +
                   MentHlth +
                   PhysHlth + 
                   Sex +
                   age_levels +
                   edu_levels +
                   inc_levels + 
                   health_levels,
                 data = train_set,
                 method = "glm",
                 family = "binomial",
                 trControl = control,
                 metric = "logLoss")

# all variables
logreg3 <- train(Diabetes_binary ~ 
                   .,
                 data = train_set,
                 method = "glm",
                 family = "binomial",
                 trControl = control,
                 metric = "logLoss")

# log reg resamples
logreg_resamples <- resamples(list(health_data = logreg1, questionnaire_data = logreg2, all_data = logreg3))

bwplot(logreg_resamples)
```

After training all three of our candidate models, the model using all the predictors minimizes log loss. However, the difference between the best logistic regression model and the simplest logistic regression model isn't too big... so we'll use the full model and the simplest model captured by `health_data` on the test set and see what the difference is.

For curiousity's sake, let's look at a summary of that best logistic regression model, and the simplest model.

```{r best logreg summary}
# simplest model
summary(logreg1)
# full model
summary(logreg3)
```

Pretty interesting.

Now we'll look at classification trees.

### Classification trees

Classification trees are non-parametric models that make predictions by partitioning the predictor space such that some loss function of interest is minimized. Predictions for the partitions, in the classification case, are simply the most prevalent class (or the proportion of that most prevalent class, if we're predicting probabilities) within that partition. Classification trees can easily overfit data, so we adjust their performance by tuning a complexity parameter `cp` for these models.

We might use classification trees when we are unsure of what variables have useful relationships with the response or when we are unsure of the exact nature of the relationship, when we are unsure of any interaction effects, and when we are interested in feature selection while still maintaining a degree of interpretability. This is only applicable with smaller predictor sets however. Also, classification tree results can be highly variable with small changes in the data, so this is another consideration we must take.

Let's fit some trees! We'll plot the `logLoss` value per value of `cp`, the complexity parameter to get an idea of which tree does best. We won't be visualizing a tree diagram though, as with all the variables we have, it'll be pretty difficult to interpret.

```{r cart}

tree_grid <- data.frame(cp = seq(from = 0.0001, to = 0.0015, by = 0.0001))

set.seed(456)
rpart_fit <- train(Diabetes_binary ~ ., data = train_set,
                  trControl = control, 
                  method = "rpart", 
                  metric = "logLoss",
                  tuneGrid = tree_grid)

plot(rpart_fit)
```

We see our estimate of best `rpart` model is at a `cp` value of `{r} rpart_fit$bestTune`.

Finally, we'll look at random forest models, which are ensembles of individual tree models.

### Random forest

**PLEASE NOTE**: Given the size of the data, instead of `method = 'rf'`, we'll be using `method = 'ranger'` to use the `ranger` package's [implementation of the random forest algorithm](https://www.jstatsoft.org/article/view/v077i01). It's much faster, and while going through this assignment, it took about 20 minutes to finish 3-fold CV over 500 trees, 3 values of mtry, and 2 values of splitrule. A prior approach on `rf` took about 2 hours for 5 values of mtry and 5-fold CV on 300 trees.

Random forest models are ensemble models. This means they use many submodels to make "averaged" predictions. The benefit here is that the prediction accuracy is typically improved by virtue of reduced variance of individual tree predictions, since predictions are aggregated from several trees.

Random forests take a bootstrapped sample, the same size as the actual sample, and train a tree on this resample. Then this tree calls predictions for some given x-values. This process is repeated a set number of times, typically 100 or 1000. Finally, those 100 or 1000 sets of predictions are averaged. This is the **bagged tree** approach.

The extra step random forests add is random sampling of the predictors to decorrelate the trees. This makes it so that correlated predictors don't dominate the predictor space.

We'll train some random forest models and select the best one based on the best `mtry` parameter. The `ranger` implementation of random forest, by default, uses 500 trees. We'll use all predictors as initial inputs. *Due to the length of time it takes to train a random forest model*, we'll be reducing the number of cross-validation folds to 3, and let the `caret`'s `train` function randomly select hyperparameter values to tune over.

```{r rf}

# 3 fold cross validation and log loss for train control
rf_control <- trainControl(method="cv", number=3, classProbs=TRUE, summaryFunction=mnLogLoss)

set.seed(456)
rf_fit <- train(Diabetes_binary ~ ., data = train_set,
                  trControl = rf_control, 
                  method = "ranger", 
                  metric = "logLoss",
                  tuneLength = 3
                  # tuneGrid = tree_grid
                )

plot(rf_fit)
```

We see from the plot of hyperparameter value relationships on CV logLoss that fewer predictors and a gini split-rule yield the best RF model from the `ranger` package.

Time to evaluate performance on the test set!

### Test set evaluation

We'll create test set predictions for the full-variable logistic regression model, for the simplest logistic regression model, for the best classification tree model, and for the best random forest model. Then we'll determine which model performs best overall, but note that we'll select the simplest model if there isn't much difference in performance between the best and second best model.

We'll create a helper function `getLogLoss` to evaluate model performance on the test set, then stick the results in a data frame and see which model gets the lowest test set log loss.

```{r test set evals}

getLogLoss <- function(model_fit, newdata) {
  
  # predict classes
  class_preds <- predict(model_fit, newdata = newdata)
  classes <- levels(class_preds)
  
  # predict probabilities
  prob_preds <- predict(model_fit, newdata = newdata, type = "prob")
  
  # put data in data frame with required columns for mnLogLoss function
  preds_df <- data.frame(obs = newdata$Diabetes_binary, pred = class_preds) |> bind_cols(prob_preds)
  
  # get log loss
  mnLogLoss(preds_df, lev = classes)
}

model_performance <- data.frame(
  models = c("full logistic regression", "simplest logistic regression", "class. tree", "random forest"),
  test_log_loss = c(
    getLogLoss(logreg3, test_set), 
    getLogLoss(logreg1, test_set), 
    getLogLoss(rpart_fit, test_set), 
    getLogLoss(rf_fit, test_set))
  )

# Viewing model results by lowest log loss
model_performance |> arrange(test_log_loss)
```

We see the best model, evaluated on the test set, is `{r} model_performance |> filter(test_log_loss == min(test_log_loss)) |> pull(models)`!

The differences between the models aren't huge, though, and we prefer simplicity, so going forward, we'll choose the simplest logistic regression model for our API. Please note that if we absolutely had to get the most accurate model, we'd likely devote more time to hyperparameter tuning on random forest, or we'd take the full logistic regression model.

## Modeling conclusion

We trained 3 logistic regression models with different predictor sets, a few classification trees, and a few random forest models using cross validation. Then we evaluated their performance on the test set. We found `{r} model_performance |> filter(test_log_loss == min(test_log_loss)) |> pull(models)` performs best.

But as we saw, the differences in test set performance aren't massive, and we prefer parsimony in our modeling, so we'll use the simplest logistic regression for our API.

Thank you for reading!
